{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26c8b3de-0894-467f-b5f5-472f2eaf8d16",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9c933c-a790-4295-8152-715e1ab04cf0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Lecture 5: Deep Learning\n",
    "\n",
    "* Convolutional networks and the Imagenet competition\n",
    "* Dropout, Batch Normalization\n",
    "* Why do deep networks generalize well?\n",
    "    * Deep networks easily fit random labels.\n",
    "    * Implicit regularization and memorization.\n",
    "\n",
    "_Recommended readings and watching_:\n",
    "\n",
    "* [Deep Learning](https://www.deeplearningbook.org/) book by Ian Goodfellow, Yoshua Bengio and Aaron Courville\n",
    "* [Understanding deep learning](https://udlbook.github.io/udlbook/) by Simon J.D. Prince\n",
    "* [Dive into Deep Learning](https://d2l.ai/), an interactive deep learning book with code, math, and discussions\n",
    "* You may want to have a look at Andrey Karpathy's [Recipe for Training Neural Networks](https://karpathy.github.io/2019/04/25/recipe/)\n",
    "* Karpathy's excellent youtube playlist [Neural Networks: Zero to Hero](https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ)\n",
    "* [Awesome Deep Phenomena](https://github.com/MinghuiChen43/awesome-deep-phenomena), a curated list of papers of interesting empirical study and insight on deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2c60b8-9bc4-4435-8435-8b80b9043566",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## <center> The mechanics of deep representations </center>\n",
    "\n",
    "<center><img src=\"figs/nucleation_resnet.png\" width=700/></center>\n",
    "\n",
    "Modified from [Hierarchical nucleation in deep neural networks](https://proceedings.neurips.cc/paper_files/paper/2020/hash/54f3bc04830d762a3b56a789b6ff62df-Abstract.html) (Neurips, 2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ffc0ae-0ac8-4986-b926-659dce749224",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Deep learning is _almost_ all about going down gradients\n",
    "\n",
    "Library we will be using: <img src=\"figs/pytorch_logo.png\" width=80 />\n",
    "\n",
    "Install pytorch from https://pytorch.org/:\n",
    "\n",
    "<center><img src=\"figs/pytorch_install.png\" width=300 /></center>\n",
    "\n",
    "and go through the tutorial [Getting Started with PyTorch](https://docs.pytorch.org/tutorials/beginner/basics/intro.html) to get ready for the exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d03d2bb-3bdf-4ebe-b2da-21bb40b1f098",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## A first look at two popular datasets:\n",
    "\n",
    "### CIFAR:\n",
    "https://app.activeloop.ai/activeloop/cifar10-train/firstdbf9474d461a19e9333c2fd19b46115348f\n",
    "\n",
    "#### Imagenet:\n",
    "https://app.activeloop.ai/activeloop/imagenet-train/firstdbf9474d461a19e9333c2fd19b46115348f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934fea88-9091-415b-96d3-5605f87494f5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# <center> Convolutional networks </center>\n",
    "\n",
    "* Units in a convolutional layer are organized in **feature maps**.\n",
    "* Each unit in a feature map is connected to local patches in the feature maps of the previous layer through a **common** set of weights called a **filter bank** (**weight sharing**). Different feature maps in a layer use different filter banks.\n",
    "\n",
    "  The result of this local weighted sum is then passed through a non-linearity such as a ReLU.\n",
    "  \n",
    "* **(max) pooling**: Downsample each feature map (not adaptive).\n",
    "\n",
    "  Each pooling unit computes the max/avg of a local patch in a feature map\n",
    "\n",
    "  This induces (approximate) translational invariance.\n",
    "* Repeat many times..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e192b3cc-5dc6-4bf1-ac56-87a5c6ec9611",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# <center> A bit of history </center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3f7c5995-ffe2-433e-80ce-e7fb457332c6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### The _ancient history_ of deep learning: Fukusima's neocognitron\n",
    "\n",
    "<center><img src=\"figs/neocognitron.png\" width=700 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5b1a7a-85ba-42f6-b9d9-efc23566674b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### The _not so ancient history_ of deep learning: LeNet\n",
    "\n",
    "<center><img src=\"figs/lenet.svg\" width=700 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8867f2db-7a00-40af-a994-f369f9c00caf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Convolution layer\n",
    "\n",
    "In one dimension, for an input image $x_j$ and a one-dimensional feature array $y_i$. In a FC layer we have: $y_i =h\\left(\\sum_j w_{ij}x_j\\right)$. With **weight sharing**:\n",
    "$$y_i = h\\left(\\sum_{j=i-K}^{i+K} w_{j-i} x_j\\right)$$\n",
    "So $x_i$ and $y_j$ have the same connection as $x_{i+a}$ and $y_{j+a}$ (which is $w_{j-i}$).\n",
    "##### Have a look at: [class torch.nn.Conv1d](https://docs.pytorch.org/docs/stable/generated/torch.nn.Conv1d.html)\n",
    "\n",
    "In two dimensions, for an input image $x_{j,j'}$ and a two-dimensional feature array:\n",
    "$$y_{i,i'} = h\\left(\\sum_{j=i-K}^{i+K}\\sum_{j'=i'-K}^{i'+K} w_{j-i,j'-i'}x_{j,j'}\\right)$$\n",
    "\n",
    "##### Have a look at: [class torch.nn.Conv2d](https://docs.pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17902245-080f-4564-b29a-bbecdfd99217",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Working with dataloaders, images and convolutions in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9b67515-c2ee-4b00-84ec-1f18aef5cdc7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define a simple imshow procedure\n",
    "def show_images(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "# load CIFAR dataset\n",
    "\n",
    "# write your own data folder here and set download = True in the torchvision dataset\n",
    "data_folder = '/home/ai/repos/multiple_readout/data/'\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "# define transforms and dataloaders\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root=data_folder, train=True,\n",
    "                                        download=False, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root=data_folder, train=False,\n",
    "                                       download=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97613c64-083f-4a71-9373-9f927a078a53",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "Have a look at some images in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6019cf-f2c8-4638-910e-29093dc9b011",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(12,4))\n",
    "images1, labels1 = next(iter(trainloader))\n",
    "show_images(torchvision.utils.make_grid(images1))\n",
    "print(\"labels:\")\n",
    "print(' '.join(f'{classes[labels1[j]]}' for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2784c1d1-bce2-417e-8b3d-e7f05b4b7a26",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "images1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bd7a0e-3bb8-4366-b9d0-ebf365784f5d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# visualize the red - green - blue content of a single image\n",
    "plt.figure(figsize=(10,4))\n",
    "cmaps = [\"Reds\", \"Greens\", \"Blues\"]\n",
    "plt.subplot(1,4,1)\n",
    "plt.imshow(np.transpose(images1[0]*0.5+0.5,(1,2,0)))\n",
    "for ic in range(3):\n",
    "    plt.subplot(1,4,ic+2)\n",
    "    plt.imshow(images1[0,ic]*0.5+0.5, cmap=cmaps[ic])\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867e8dba-3ccf-4f4e-8ce2-9c1886210142",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "## Convolution with fixed filters: an example with edge detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a897fa7-cb71-41d4-acbc-691ea36e06b5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "#### A torch rendition of the wiki article [Sobel operator](https://en.wikipedia.org/wiki/Sobel_operator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2d771d-7deb-4ab9-a9a3-064ec01ea620",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read from file and transform into torch tensor\n",
    "image = transforms.ToTensor()(Image.open(\"figs/bikesgray.jpg\"))\n",
    "\n",
    "# show image (first dimension is its unique color channel)\n",
    "plt.imshow(image[0].numpy(), cmap=\"grey\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293c5192-1c75-4bd7-8156-4ceb2466540a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "Define the Sobel kernels and use torch to compute convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa9872c-3bc8-4fdb-950a-f2b5a0caff0b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Gx (horizontal gradient)\n",
    "Gx = torch.tensor([\n",
    "    [-1., 0., 1.],\n",
    "    [-2., 0., 2.],\n",
    "    [-1., 0, 1.]\n",
    "])\n",
    "\n",
    "# Gy (vertical gradient)\n",
    "Gy = torch.tensor([\n",
    "    [-1., -2., -1.],\n",
    "    [ 0.,  0.,  0.],\n",
    "    [ 1.,  2.,  1.]\n",
    "])\n",
    "\n",
    "# stack the two filters and unsqueeze\n",
    "weights = torch.stack([Gx, Gy])\n",
    "weights = torch.unsqueeze(weights,1) # add an empty dimension for group convolution in conv2d\n",
    "\n",
    "# Apply convolution using the fixed weights\n",
    "filtered_image = F.conv2d(image, weight=weights, padding=1) # padding = 1 is used to keep dimension equal (size of kernel is 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892f3c37-3048-437e-8be5-983b1092ff9b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "Have a look at the filterd images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d168e93-248d-4df3-ac19-6a088754e6c5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(131)\n",
    "plt.imshow(filtered_image[0].numpy(), cmap='grey');\n",
    "plt.subplot(132)\n",
    "plt.imshow(filtered_image[1].numpy(), cmap='grey');\n",
    "plt.subplot(133)\n",
    "plt.imshow(torch.sqrt(filtered_image**2).sum(0).numpy(), cmap='grey');\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cd1b60-924a-4c3d-8762-ac92c3b058b7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "#### A simpler example with a synthetic image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff44a445-fd9b-4e4c-8d34-0bef02da817b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "size = 100\n",
    "img = torch.zeros(1, size, size, dtype=torch.float32)\n",
    "\n",
    "start = size // 4\n",
    "end = 3 * size // 4\n",
    "\n",
    "img[0, start:end, start + 4:start + 8] = 1.0\n",
    "img[0, start + 20:start + 24, start:end] = 1.0\n",
    "\n",
    "plt.imshow(img[0], cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99cea60-c763-4c3e-b7f8-a53187318d2e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered_image = F.conv2d(img, weight=weights, padding=1)\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(131)\n",
    "plt.imshow(filtered_image[0].numpy(), cmap='grey', origin=\"lower\");\n",
    "plt.subplot(132)\n",
    "plt.imshow(filtered_image[1].numpy(), cmap='grey', origin=\"lower\");\n",
    "plt.subplot(133)\n",
    "plt.imshow(torch.sqrt(filtered_image**2).sum(0).numpy(), cmap='grey', origin=\"lower\");\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5a489e-25b2-403d-87bd-fadd74b5453c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "Let's look at a horizontal section of the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af7f163-1785-4fb9-9d76-e63155232f2d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(img[0,start]);\n",
    "plt.plot(filtered_image[0,start]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2e162f-2ef0-4b57-9db0-bbc47e1a7a15",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "#### Food for thought: taking derivatives through integrals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080c0914-8ddb-4c76-b80a-218413c75d80",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "epss = np.linspace(0.1, 0.01, 5)\n",
    "\n",
    "def heat(x, eps):\n",
    "    return np.exp(-0.5 * x**2 / eps) / np.sqrt(2*np.pi*eps)\n",
    "\n",
    "def diffheat(x, eps):\n",
    "    return x/eps * np.exp(-0.5 * x**2 / eps) / np.sqrt(2*np.pi*eps)\n",
    "\n",
    "xs = np.linspace(-3, 3., 500)\n",
    "\n",
    "for iep, eps in enumerate(epss):\n",
    "    plt.plot(xs, heat(xs, eps), alpha=0.3, color='black', label=\"heat\" if iep==0 else None)\n",
    "    plt.plot(xs, diffheat(xs, eps), color='red', label=\"diff heat\" if iep==0 else None)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64b445b-7417-4943-aee1-84331827e20c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## The rise of Convolutional networks: Alexnet and the Imagenet 2012 competition\n",
    "\n",
    "ImageNet Large Scale Visual Recognition Challenge (**ILSVRC**) is an annual computer vision competition. The goal is to classify images into 1000 classes. It uses 1.2 million training images, 50.000 validation images, and 150.000 testing images.\n",
    "\n",
    "<img src=\"figs/imagenet_labels.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c5e052-83c6-45f2-99a9-9d0d25fbe2fa",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Alexnet\n",
    "\n",
    "<img src=\"figs/imagenet_architecture.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e50e78a-3f95-41b5-b0ff-99aaa273e6df",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Alexnet in detail #1\n",
    "\n",
    "The first 5 layers of weights are convolutional. The last three layers are fully connected.\n",
    "\n",
    "The first convolutional layer filters the $224\\times 224\\times 3$ input image with 96 kernels (feature maps) of size $11\\times 11\\times 3$ with a stride of 4 pixels (this is the distance between the receptive field centers of neighboring neurons in a kernel map).\n",
    "\n",
    "The second convolutional layer takes as input the (response-normalized and pooled - See [paper](https://papers.nips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html) for details) output of the first convolutional layer and filters it with 256 kernels of size $5\\times 5 \\times 48$.\n",
    "\n",
    "The third convolutional layer has 384 kernels of size $3\\times 3\\times 256$.\n",
    "The fourth convolutional layer has 384 kernels of size $3\\times 3 \\times 192$.\n",
    "The fifth convolutional layer has 256 kernels of size $3\\times 3 \\times 192$.\n",
    "(No pooling or normalization).\n",
    "\n",
    "In layers 2, 4, and 5, kernel computation on each GPU takes input only from its own GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3f732a-d348-4371-bfdc-2487da9ca456",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Alexnet in detail #2\n",
    "\n",
    "The fully-connected layers each have 4096 neurons.\n",
    "\n",
    "The output of the last fully-connected layer is fed to a 1000-way softmax which produces a distribution over the 1000 class labels.\n",
    "\n",
    "All non-linearities are **ReLU**.\n",
    "\n",
    "The network is trained to maximize the **multinomial logistic regression** objective.\n",
    "\n",
    "It has **60 million parameters** and 650,000 neurons.\n",
    "\n",
    "Pre-trained available on [pytorch hub](https://pytorch.org/hub/pytorch_vision_alexnet/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb58cf3-dfd9-4110-839f-da2de4ba11dc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Alexnet in detail #3\n",
    "\n",
    "**Data augmentation**:\n",
    "During training, each $256\\times 256$ image is replaced by 2048 images of size $224\\times 224$ (shifting and reflection).\n",
    "At test time, the output is computed for 5 images of size $224\\times 224$ (four corners and center) and the results are averaged.\n",
    "\n",
    "**Dropout** is used in the first two fully-connected layers. Without dropout, there is substantial overfitting. Dropout roughly doubles the number of iterations required to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e931506-42bd-4656-91ea-a1c0df167479",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Convolutional filters found by Alexnet\n",
    "\n",
    "Filters resulting from training:\n",
    "\n",
    "<center><img src=\"figs/filters_imagenet_architecture.png\" /></center>\n",
    "\n",
    "##### Have a look at [Gabor](https://en.wikipedia.org/wiki/Gabor_filter) filters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f30167-5269-4c69-99f6-6132ad0421e4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## <center> (Deep) learning is all about generalization </center>\n",
    "\n",
    "#### How can we understand the good generalization performance of deep learning?\n",
    "\n",
    "Traditionally, good generalization is explained by:\n",
    "* The structure in the data matches the structure in the model.\n",
    "* Sufficient data.\n",
    "\n",
    "Deep networks can essentially learn anything and often have insufficient data. How can we understand their good generalization performance?\n",
    "\n",
    "Here we look at:\n",
    "* Dropout, regularization, data augmentation, batch normalization.\n",
    "* Implicit regularization of GD and memorization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1384b4ca-8c25-445d-b505-9f5e9e3b5816",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Dropout\n",
    "\n",
    "Combining the predictions of many different models is a very successful way to reduce test errors.\n",
    "\n",
    "'Dropout' mimics **model averaging** by setting the output of each hidden neuron to zero with a probability of 0.5. The neurons that are dropped out in this way do not contribute to the forward pass and do not participate in backpropagation. Every time an input is presented, the neural network samples a different architecture, but all these architectures share weights.\n",
    "\n",
    "This technique reduces **complex co-adaptations** of neurons, since a neuron cannot rely on the presence of particular other neurons. It is, therefore, forced to learn **more robust features** that are useful in conjunction with many different random subsets of the other neurons.\n",
    "\n",
    "At test time, you can either:\n",
    "* For each test pattern, average the output for random dropout realisations.\n",
    "* Use all the neurons but multiply their outputs by 0.5.\n",
    "\n",
    "##### Have a look at [torch.nn.Dropout](https://docs.pytorch.org/docs/stable/generated/torch.nn.Dropout.html) and the original [paper](https://jmlr.org/papers/v15/srivastava14a.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c19e6ea-7508-44dc-8602-057f38502a46",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Batch normalization (BN)\n",
    "\n",
    "A $T$-layer network:\n",
    "$$y_k = h\\left(\\sum_{j_T} w^{(T)}_{j_Tj_{T-1}}h\\left(\\sum_{j_{T-1}} w^{(T-1)}_{j_{T-1}j_{T-2}}h\\left(\\ldots h\\left(\\sum_{j_1}w_{j_1j_0}^{(1)} x_{j_0}\\right)\\right)\\right)\\ldots\\right)$$\n",
    "with $h$ a non-linear activation function.\n",
    "Or,\n",
    "$$x^{(t)}=w^{(t)} a^{(t)} \\qquad a^{(t+1)}=h\\left(x^{(t)}\\right) \\qquad t=1,\\ldots, T$$\n",
    "with $x^{(t)}, a^{(t)}$ being the vectors of summed input and output neural activity, and $w^{(t)}$ being the $t$-th layer weight matrix.\n",
    "\n",
    "**Vanishing gradient problem:**\n",
    "Empirically, it's observed that $w^{(t)}$ tends to grow as a result of learning. Then $h\\left(x^{(t)}\\right)$ saturates.\n",
    "The gradient $h'\\left(x^{(t)}\\right)\\to 0$, which slows down learning.\n",
    "This problem is largest in early layers of deep networks.\n",
    "\n",
    "**Co-dependence of learning in different layers:**\n",
    "Since $x^{(t)}=w^{(t)}h\\left(w^{(t-1)} a^{(t-1)}\\right)$, the optimal value of $w^{(t)}$ depends on the optimal values of $w^{(t-1)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e116eb0a-bc41-4c9d-9fcc-a8cb90182d3b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Batch normalization and internal covariate shift\n",
    "\n",
    "Batch normalization addresses the issue of _Internal Covariate Shift_ by normalizing the inputs to each layer within a mini-batch.\n",
    "\n",
    "<center><img src=\"figs/batch_normalization_algorithm.png\" width=400></center>\n",
    "\n",
    "##### Have a look at [torch.nn.BatchNorm2d](https://docs.pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html) and the original [paper](https://proceedings.mlr.press/v37/ioffe15.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f58325c-0c5b-4891-a21b-27d64111038a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# BatchNorm in pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a4829e-50f7-4127-81d0-ac7238995d49",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "Let us extract a new batch of images so that we can test the update of running statistics in the BatchNorm layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d7b7bde-e7c2-482e-9a65-452f74e6384d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "images2, labels2 = next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29178d79-4595-4e40-9366-48f80da34e98",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "images1.shape, images2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725b9951-5093-4e22-9736-67517046907d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "We will look at the effect of BN after a fully connected layer (without bias for simplicity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96402636-856b-4d02-b2aa-097cb811c1d4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "in_features = np.prod(images1.shape[-2:])\n",
    "out_features = 200\n",
    "fc = nn.Linear(in_features, out_features, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5e3e84-acfa-43d1-9f0e-a616f8e9b037",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fc.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80a34eff-c187-4343-83ca-57b7455b6e22",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# flatten the image before passing to the FC layer\n",
    "output_fc1 = fc(images1.view(-1, 3, in_features))\n",
    "output_fc2 = fc(images2.view(-1, 3, in_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adb118f-af17-4ba7-9914-e855b1b643ac",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# note that channels are computed in parallel\n",
    "output_fc1.shape, output_fc2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1369ab7b-ed95-468c-ba3f-ca557d2c4a53",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "Let us implement the Linear layer ourselves and check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b457c0-488c-4cb0-9eb9-97fb09d0880e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_fc1_mine = torch.einsum('nci,ki->nck', images1.view(-1, 3, in_features), fc.weight)\n",
    "output_fc2_mine = torch.einsum('nci,ki->nck', images2.view(-1, 3, in_features), fc.weight)\n",
    "\n",
    "torch.allclose(output_fc1, output_fc1_mine),\\\n",
    "torch.allclose(output_fc2, output_fc2_mine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a00f04-24c4-4ebf-a367-09b863171387",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "Let's define a BatchNorm object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b94a3f5-7312-4e45-98fd-e30d9c1be779",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "momentum = 1.\n",
    "m = nn.BatchNorm1d(3, eps=1e-100, momentum=momentum, affine=True, track_running_stats=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4918b6-8963-444a-89dd-949e5d6b9562",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "The running average is performed using\n",
    "\n",
    "$$\\hat{x}_{\\text{new}}=\\left(1-\\text{momentum}\\right)\\times\\hat{x}_{\\text{old}}+\\text{momentum}\\times x_{t}$$\n",
    "\n",
    "where $\\hat{x}$ stands for the estimate of average and variance.\n",
    "\n",
    "As they note in Dive into Deep Learning:\n",
    "\n",
    "`This is somewhat of a misnomer as it has nothing whatsoever to do with the momentum term of optimization`\n",
    "\n",
    "and it also acts in the opposite way as how you would think momentum does!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd21d0e-0a53-49ef-b0d1-48e00fcd7e17",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "m.train() # this is needed in order to track the running stats and allow learning of the affine transormation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4237f15d-c38a-4938-b506-cc14ba970765",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "Pass the two FC outputs through the BN layer and look at the resulting shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9d35bc-bea7-47e7-af08-e5f35bf81fd5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_bn1 = m(output_fc1)\n",
    "output_bn2 = m(output_fc2)\n",
    "\n",
    "output_bn1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67510a9d-9b17-4912-8f69-76e43afae830",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "Have a look at how the distribution changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9674e21-f458-47de-8d23-4818e0358599",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.hist(output_fc1.detach().numpy().flatten(), bins=100, density=True, histtype=\"step\", label=\"after fc\");\n",
    "plt.hist(output_bn1.detach().numpy().flatten(), bins=100, density=True, histtype=\"step\", label=\"after bn\");\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e44e22-793b-45ab-953b-ad7f33fdd7bd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "Check the new batch average and variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63176155-82f9-4b3a-8158-af8d0a5366e6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_bn1.mean([0,2]), output_bn1.var([0,2], correction=0) ,\\\n",
    "output_bn2.mean([0,2]), output_bn2.var([0,2], correction=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc096f0-3411-4470-991a-24955595321b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "Look at the running stats stored inside BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8048f15-1696-4dad-8b92-fa2a8ecef22e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "m.running_mean, m.running_var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7754c2c1-51c0-4143-9ce7-291b9a871ee1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "and compute them as a check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1533f11-98ac-4507-8c34-cb96f96db4be",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "(1 - momentum) * momentum * output_fc1.mean([0,2]) + momentum * output_fc2.mean([0,2]), \\\n",
    "(1 - momentum) * momentum * output_fc1.var([0,2]) + momentum * output_fc2.var([0,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a68d702-88a5-4ca8-a056-61da9893df30",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "Instantiate random outputs to backprop through BN and look at the resulting gradients over $\\gamma$ and $\\beta$ parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027631ab-2178-4933-8027-ceba28ebbf5b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "yhat1 = torch.tanh(output_bn1).sum((1,2))\n",
    "yhat2 = torch.tanh(output_bn2).sum((1,2))\n",
    "\n",
    "y1 = torch.randn(output_bn1.shape[0])\n",
    "y2 = torch.randn(output_bn2.shape[0])\n",
    "\n",
    "loss = ((yhat1 - y1)**2).mean() + ((yhat2 - y2)**2).mean()\n",
    "loss.backward()\n",
    "\n",
    "for name, p in m.named_parameters():\n",
    "    print(name, p)\n",
    "    print(p.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6f1784-3002-4456-9bfd-8352aeea1fd7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "Now let's run it at evaluation time on a different set of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f1567c-05b7-4784-a592-c2327d6607fd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "images3, labels3 = next(iter(trainloader))\n",
    "\n",
    "m.eval()\n",
    "with torch.no_grad():\n",
    "    output_fc3 = fc(images3.view(-1, 3, in_features))\n",
    "    output_bn3 = m(output_fc3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3744259-898c-4277-868a-53ae0addf4c7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "Note that eval() stops running stats from being updated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec600a97-81c7-4b93-b5a4-ba9866f41c00",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "m.running_mean, m.running_var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd62e7f-8c2e-41a0-a4dd-d4566e68364e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "Check outputs of BN with what you would expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a8c514-370b-4df4-aab4-12ad5b7db93f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "((output_fc3 - m.running_mean[None,:,None]) / torch.sqrt(m.running_var[None,:,None])).mean((0,2)),\\\n",
    "output_bn3.mean((0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764dbe51-17fd-4321-8a72-9fb30e1c9dea",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "((output_fc3 - m.running_mean[None,:,None]) / torch.sqrt(m.running_var[None,:,None])).var((0,2)),\\\n",
    "output_bn3.var((0,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1913fd7e-0208-48da-934d-04d7caa43fe1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Batch normalization accelerates training - exibit #1\n",
    "\n",
    "BN accelerate training and stabilizes activity distributions.\n",
    "\n",
    "<center><img src=\"figs/batch_normalization_mnist.png\" width=600></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2c6ff5-74c2-457b-b292-fc2b1d450c67",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Batch normalization accelerates training - exibit #2\n",
    "\n",
    "BN-Inception architecture achieves better accuracy faster with:\n",
    "* no dropout\n",
    "* larger learning rate (x5 or x30)\n",
    "* faster learning rate decay\n",
    "* smaller regularization \n",
    "\n",
    "<center><img src=\"figs/batch_normalization_inception.png\" width=700></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccc6be8-a94c-4771-bdbd-0ae29b674035",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Deep neural networks easily fit random labels\n",
    "\n",
    "<center><img src=\"figs/zhang_random_labels.png\" width=600></center>\n",
    "\n",
    "#### Main conclusions:\n",
    "* deep networks can easily learn when labels are randomly shuffled\n",
    "* regularization does not hurt learning with random labels (does not prevent overfitting)\n",
    "* generalization is good even without regularization (although regularization helps)\n",
    "\n",
    "##### Have a look at the original [paper](https://arxiv.org/abs/1611.03530) for more details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ea1ad2-3f0f-47cb-a2b9-669551c63a39",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Generalization: CIFAR\n",
    "\n",
    "<center><img src=\"figs/zhang_table1_cifar.png\" width=600></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bcc638-c6a6-4ae6-bac7-66e9194caaf0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Generalization: Imagenet\n",
    "\n",
    "<center><img src=\"figs/zhang_table2_imagenet.png\" width=600></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0003cc9d-ed0e-4d9b-a30d-c68593b0e74f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Learning dynamics and implicit regularization\n",
    "\n",
    "<center><img src=\"figs/zhang_learning_dynamics.png\" width=800></center>\n",
    "\n",
    "#### Main conclusions:\n",
    "* Data augmentation may yield significantly better generalization.\n",
    "* Early stopping works in some cases (Imagenet) but not in others (CIFAR).\n",
    "* Batch normalization (BN) may significantly improve convergence and generalization on Cifar10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd38dc09-cccd-4455-ad6a-ab1334da3d28",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Implicit regularization of gradient descent\n",
    "\n",
    "When the number of samples $N$ is smaller than the number of parameters in the network and is trained with (Stochastic) Gradient Descent (SGD), one observes **memorization**: the network explicitly represents the training data in some form.\n",
    "\n",
    "We illustrate this for the problem of linear regression\n",
    "$$y=\\sum_{i=1}^d w_i x_i$$\n",
    "\n",
    "The problem can be easily generalized to higher-dimensional output, non-linear output, and a more general cost function, where $y_j=\\phi(\\sum_{i=1}^d w_{ij} x_i)$ with $\\phi$ an invertible function (such as sigmoid, for instance) and $L(w)= \\sum_\\mu F(y^\\mu,t^\\mu) $ with $F(y,t)\\ge 0 $ and such that $F(y,t)=0$ if $y=t$ and $y_j^\\mu =\\phi\\left(\\sum_{i=1}^d w_{ij} x^{\\mu}_i\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4832b9-1dc2-418e-8f1c-cd1e6649a916",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Implicit regularization of gradient descent\n",
    "\n",
    "Given the data $\\{x^{\\mu}_i,t^\\mu,\\mu=1,\\ldots,N\\}$ and cost:\n",
    "$$L(w)=\\frac{1}{2}\\sum_\\mu (y^\\mu -t^\\mu)^2 \\qquad y^\\mu =\\sum_{i=1}^d w_i x^\\mu_i$$A solution $w$ with $L(w)=0$ satisfies:$$t^\\mu=y^\\mu =\\sum_i w_i x_i^\\mu \\qquad t = X w$$\n",
    "with $X_{(N\\times d)}=x^\\mu_i$ and $t_{(N\\times 1)}=t^\\mu$.\n",
    "\n",
    "We have $N$ equations and $d$ unknowns. When $N< d$, there are many solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baecfdcf-b9af-4563-8503-ca117596e650",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Implicit regularization of gradient descent\n",
    "\n",
    "The gradient is a linear combination of the training data:\n",
    "$$\\frac{\\partial L}{\\partial w_i}=\\sum_\\mu (y^\\mu -t^\\mu) x_i^\\mu \\qquad \\Delta w_i^t = -\\eta_t \\frac{\\partial L}{\\partial w_i}$$Therefore, starting at $w=w^0$, the (S)GD solution is a linear combination of the training data:$$w_i = w^0_i+\\sum_{t=1}^T \\Delta w_i^t = w^0_i+\\sum_\\mu \\alpha_\\mu x_i^\\mu\\qquad w=w_0+X^T \\alpha$$with $\\alpha_{(N\\times 1)}=\\alpha^\\mu$. Since also $X w =t$, we obtain:$$XX^T \\alpha =t -X w_0$$\n",
    "Since $N<d$, $XX^T$ is full rank and invertible.\n",
    "Thus, gradient descent finds a unique solution:\n",
    "$$w =w_0+X^T \\alpha \\qquad \\alpha = \\left(XX^T\\right)^{-1}\\left( t -Xw_0\\right)$$\n",
    "but it depends on the initialization $w_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fe6d42-089b-402e-bd36-0c049155ec2f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Implicit regularization of gradient descent\n",
    "\n",
    "When $w_0=0$, the (S)GD solution is also the minimum norm solution:\n",
    "$$\\min_w \\frac{1}{2}\\|w\\|^2 \\qquad \\text{s.t} \\quad X w = t$$\n",
    "Introducing Lagrange multipliers, we get:\n",
    "$$\\mathcal{L}=\\frac{1}{2}\\|w\\|^2 +\\sum_\\mu \\lambda^\\mu \\left( t^\\mu - \\sum_i w_i x^\\mu_i\\right)$$\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial w_i}= w_i - \\sum_\\mu \\lambda^\\mu x^{\\mu}_{ i}=0$$\n",
    "Thus, $w=X^T \\lambda$ and $Xw=t$. The solution is identical to the GD solution with $\\lambda=\\alpha$ and $w_0=0$.\n",
    "\n",
    "The minimum norm solution is the most _smooth_ solution consistent with training error zero. Therefore, (S)GD on **over-parametrized** problems has an implicit regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3516517-63b5-4895-9433-5829e90e609d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Gradient descent solution memorizes the training set\n",
    "\n",
    "The final solution is:\n",
    "$$y=\\sum_i w_i x_i = \\sum_{i,\\mu} \\alpha^\\mu x_i^\\mu x_i$$\n",
    "By construction, when $x=x^\\mu$, the output $y=t^\\mu$.\n",
    "\n",
    "For $x\\approx x^\\mu$, the output $y\\approx t^\\mu$.\n",
    "\n",
    "The solution 'memorizes' the training set and is expected to give good generalization for data (very) near individual training samples and bad generalization everywhere else.\n",
    "\n",
    "So the answer to the question: why do deep neural networks generalize well is that they are tested on data near the training samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5c4c2e-2a2c-4462-a1e5-58afa14a2003",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Memorization in Overparametrized Autoencoders\n",
    "\n",
    "We can further understand the properties of deep networks by studying iterated autoencoders.\n",
    "\n",
    "Consider an autoencoder defined by a function $f: \\mathbb{R}^d \\to \\mathbb{R}^d$. After successful training, all training samples satisfy $f(x^\\mu)=x^\\mu$.\n",
    "\n",
    "Consider the dynamical system obtained by iterating the autoencoder multiple times: $x_{t+1}=f(x_t)$.\n",
    "The training examples are **local attractors**, i.e., stable fixed points of the dynamics, if for $x_0$ in a neighborhood of $x^\\mu$ we have:\n",
    "$$x_0 \\to x_1=f(x_0) \\to x_2 = f(x_1) \\to \\ldots x_\\infty=x^\\mu$$\n",
    "\n",
    "Local stability requires that all eigenvalues $\\lambda_i$ of the Jacobian\n",
    "$$J_{ij}=\\frac{\\partial f_i}{\\partial x_j}(x^\\mu)$$\n",
    "satisfy $|\\lambda_i|<1$. Otherwise, the point is unstable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d39f6ce-2df4-4bd7-b3fe-21faa11c2dee",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Memorization in (fully connected) overparametrized Autoencoders - exibit #1\n",
    "\n",
    "When $f$ is an overparameterized autoencoder, trained with (S)GD, the training examples are not only fixed points, but also attractors.\n",
    "\n",
    "<center><img src=\"figs/autoencoder_swissroll.png\" height=2000></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dbf5ff-9abd-49e9-bcf7-843a67b94040",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Memorization in (fully connected) overparametrized Autoencoders - exibit #2\n",
    "\n",
    "<center><img src=\"figs/autoencoder_mnist.png\" width=1000></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cc6af1-54a3-41f2-9c36-139beb794a6b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Memorization in (deep convolutional) overparametrized Autoencoders\n",
    "\n",
    "<center><img src=\"figs/autoencoder_convolutional.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb5867a-b606-4249-952e-4e032f48c37b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Memorization and Generalization coexist\n",
    "\n",
    "**Memorization:** Overparameterized autoencoders exhibit a form of data-dependent self-regularization that encourages solutions that concentrate around the training examples.\n",
    "\n",
    "<center><img src=\"figs/autoencoder_robustness.png\"></center>\n",
    "\n",
    "##### Have a look at the [arxiv](https://arxiv.org/abs/1810.10333) and [PNAS](https://www.pnas.org/doi/10.1073/pnas.2005013117) versions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52681588-d781-4ecc-b524-e0b5e3a6f352",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "# <center> Assignments </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e66cb72-0679-4ee1-ba5a-d6d4a335b87e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "#### Ex 5.1\n",
    "Study the performance properties of the convolutional network provided in the introductory [pytorch tutorial](https://docs.pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html).\n",
    "\n",
    "How is learning affected if instead of ReLU units, tanh() activations are used? What is the reason for this? Compare also at least two different optimizer algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865e574d-0020-4d24-83aa-2073549d233a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "#### Ex 5.2\n",
    "Try to outperform the convolutional network of **Ex 5.1** with a multilayer perceptron (MLP) that uses approximately the same number of parameters.\n",
    "Report your results and explain them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3503970-998a-48fc-b898-dc6cb8132379",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "#### Ex 5.3\n",
    "\n",
    "Learning convolutions from scratch starting from a fully connected initialization is hard.\n",
    "\n",
    "Reproduce the emergence of localized receptive fields in a one-hidden layer fully connected network as described in [this paper](https://www.pnas.org/doi/abs/10.1073/pnas.2201854119). For simplicity, use a 1-dimensional version of the GP and NLGP synthetic data models described in the Methods sections. Use a network with $K=30$ hidden neurons, fixed second-layer weights and $\\text{erf}(x/\\sqrt{2})$ activation function. Feel free to use either a fixed dataset or work in an online learning setting, where new inputs are produced before each gradient step."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
