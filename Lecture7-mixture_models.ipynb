{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abd8eb18-2b61-4059-bb7f-3e670e2ff86c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from utils import color_cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0522c7e2-bf71-4c9d-b79a-0d292757bf6a",
   "metadata": {},
   "source": [
    "# Lecture 7: Mixture models and Expectation Maximization\t\n",
    "\n",
    "* Clustering and Latent variable models \n",
    "* Mixture models\n",
    "* Expectation Maximization\n",
    "\n",
    "Murphy **Chapter 11** (sections 11.1, 11.2, 11.3, 11.4, 11.4.1, 11.4.2, 11.4.7)\n",
    "\n",
    "_Recommended reading_:\n",
    "* Christopher Bishop's [Pattern Recognition and Machine Learning](https://www.microsoft.com/en-us/research/people/cmbishop/prml-book/) **Chapter 9**\n",
    "* Have a look at the scikit-learn demo on [Comparing different clustering algorithms on toy datasets](https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a09fd84e-0e40-4651-837c-b08cb0ab2e5e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "## A first look at a clustering problem: K-means\n",
    "\n",
    "#### Load old faithful dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b961687e-988e-4356-96be-7ba65d5fb2ad",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "points_oldfaith = np.loadtxt(\"datasets/oldFaith.txt\")\n",
    "points_oldfaith = (points_oldfaith - points_oldfaith.mean(0)[None]) / points_oldfaith.std(0)[None]\n",
    "plt.scatter(points_oldfaith[:,0], points_oldfaith[:,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7745bf-f462-4ffd-a2a7-98e6da6a99e5",
   "metadata": {},
   "source": [
    "### K-means in scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5943da64-04eb-458b-b205-672144639081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run K-means\n",
    "num_clusters = 2\n",
    "kmeans = KMeans(n_clusters=num_clusters, n_init='auto', verbose=1, random_state=1).fit(points_oldfaith)\n",
    "centers_sklearn = kmeans.cluster_centers_\n",
    "labels_sklearn = kmeans.labels_\n",
    "\n",
    "# plot clustering\n",
    "for k in range(num_clusters):\n",
    "    plt.plot(centers_sklearn[k,0], centers_sklearn[k,1], 'x', ms=10)\n",
    "    plt.scatter(points_oldfaith[labels_sklearn==k,0], points_oldfaith[labels_sklearn==k,1], alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2311fe-65bd-4719-860c-465d61b198d5",
   "metadata": {},
   "source": [
    "### K-means: do it yourself\n",
    "\n",
    "```\n",
    "initialize K centroid vectors (cluster centers) m_k\n",
    "\n",
    "for iter from 1 to max_iter or until convergence:\n",
    "\n",
    "    for mu from 1 to N:\n",
    "        # Assign each data point to its closest cluster center\n",
    "        k_mu ← arg min || x_mu − m_k ||\n",
    "\n",
    "    for k from 1 to K:\n",
    "        # Update each cluster center m_k by computing the mean of all points assigned to it\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6611c4ff-08de-423b-8d59-e0517bc52d3f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_clusters = 2\n",
    "num_steps = 100\n",
    "tol = 1e-1\n",
    "seed = 20\n",
    "keep_story = True\n",
    "\n",
    "center = None\n",
    "center = np.array([[-1,1],\n",
    "                  [1,-1.1]])\n",
    "\n",
    "# ...you will write your own implementation of k_means...\n",
    "# res_kmeans = mixture_models_code.k_means(points_oldfaith, num_clusters, num_steps, tol,\n",
    "#                                          center=center, seed=seed, verbose=True, keep_story=keep_story)\n",
    "# pickle.dump(res_kmeans, open(\"datasets/res_kmeans.p\", \"wb\" ))\n",
    "\n",
    "# load results obtained with my own kmeans implementation\n",
    "res_kmeans = pickle.load(open(\"datasets/res_kmeans.p\", \"rb\"))\n",
    "resp, center, dist_centers, distortions, resps, centers = res_kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047223cd-fe1e-43a4-9053-fb2946f8ca6d",
   "metadata": {},
   "source": [
    "#### Visualize trajectory of centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0881de-44b5-4f05-9b19-7d2815c65c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(points_oldfaith[:,0], points_oldfaith[:,1], alpha=0.2)\n",
    "for k in range(num_clusters):\n",
    "    plt.plot(centers[0,k,0], centers[0,k,1], 'o', c=color_cycle[k]);\n",
    "    plt.plot(centers[:,k,0], centers[:,k,1], '.-', c=color_cycle[k]);\n",
    "    plt.plot(centers[-1,k,0], centers[-1,k,1], 'x', ms=10, c=color_cycle[k]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cdea55-162c-460c-a2ca-7a509272c167",
   "metadata": {},
   "source": [
    "#### Visualize trajectory in details with cluster assignments through iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a94dc41-f184-41df-9e6e-3416723acbe3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tot_steps = len(resps)\n",
    "\n",
    "fig, axs = plt.subplots(tot_steps, 1)\n",
    "fig.set_size_inches(10, 3 * tot_steps)\n",
    "for step, (resp, center) in enumerate(zip(resps, centers)):\n",
    "    for k in range(num_clusters):\n",
    "        axs[step].set_aspect('equal', 'box')\n",
    "        axs[step].plot(center[k,0], center[k,1], 'x', ms=8)\n",
    "        axs[step].scatter(points_oldfaith[resp==k,0], points_oldfaith[resp==k,1], alpha=0.1)\n",
    "    for k1 in range(num_clusters):\n",
    "        for k2 in range(k1 + 1, num_clusters):\n",
    "            fromto = center[[k1,k2]]\n",
    "            middle_point = fromto.mean(0)\n",
    "            axs[step].plot(fromto[:,0], fromto[:,1], ':', c=\"black\", alpha=0.5)\n",
    "            axs[step].plot(middle_point[0], middle_point[1], 'o', c=\"black\", ms=2, alpha=0.5)\n",
    "            dk1k2 = fromto[1] - fromto[0]\n",
    "            orth_dk1k2 = np.array([-dk1k2[1], dk1k2[0]])\n",
    "            orth_dk1k2 /= np.sqrt((orth_dk1k2**2).sum())\n",
    "            axs[step].plot([middle_point[0]-orth_dk1k2[0],middle_point[0]+orth_dk1k2[0]],\n",
    "                     [middle_point[1]-orth_dk1k2[1],middle_point[1]+orth_dk1k2[1]], color=\"black\", alpha=0.5)\n",
    "            \n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6bfa96-243a-45a9-9d51-79cbbf5b7101",
   "metadata": {},
   "source": [
    "We can visualize the progression of K-means looking at a _cost function_ representing the **distortion** in a code based on the centroids only:\n",
    "\n",
    "$$\n",
    "J=\\frac{1}{N}\\sum_{i=1}^{N}\\left\\Vert x_{\\mu}-m_{k_{\\mu}}\\right\\Vert ^{2}\n",
    "$$\n",
    "\n",
    "Have a look at Murphy section 11.4.2.6 and Figure 9.2 in Bishop's book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e175317-e92b-4bda-97bc-ec0c044b1427",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "plt.subplot(121)\n",
    "colors = np.zeros((len(distortions), 3))\n",
    "colors[::2,2] = 1\n",
    "colors[1::2,0] = 1\n",
    "plt.plot(np.arange(len(distortions)), distortions, c=(0,1,0), alpha=0.5);\n",
    "plt.scatter(np.arange(len(distortions)), distortions, c=colors);\n",
    "plt.xlabel(\"E/M step\")\n",
    "plt.ylabel(\"distortion\");\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(dist_centers, '.-');\n",
    "plt.xlabel(\"steps\")\n",
    "plt.ylabel(\"center displacement\");\n",
    "\n",
    "plt.tight_layout();\n",
    "\n",
    "print(\"Left panel\")\n",
    "print(\"Blue points: compute assignments to centroids (E step)\")\n",
    "print(\"Red points: recompute centroids (M step)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3325c55a-b223-4334-b69d-76f94e7e978e",
   "metadata": {},
   "source": [
    "#### Run K-means a bunch of times and look at the attractors of the algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7ff30c-4a64-43ea-a5e6-cd2a8e857f83",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_seeds = 1000\n",
    "\n",
    "# here's how I produced the results from many different initializations\n",
    "# allcenters = []\n",
    "# for seed in range(num_seeds):\n",
    "#     center = 4 * np.random.rand(num_clusters, 2) - 2\n",
    "#     _, _, _, _, _, centers = mixture_models_code.k_means(points_oldfaith, num_clusters, num_steps, tol,\n",
    "#                                                          center=center, seed=seed, keep_story=keep_story)\n",
    "#     allcenters.append(centers)\n",
    "# pickle.dump(allcenters, open(\"datasets/allcenters.p\", \"wb\" ))\n",
    "\n",
    "# load results obtained with my own kmeans implementation\n",
    "allcenters = pickle.load(open(\"datasets/allcenters.p\", \"rb\"))\n",
    "k = 0\n",
    "for centers in allcenters:\n",
    "    plt.plot(centers[0,k,0], centers[0,k,1], '.', c=color_cycle[k], alpha=0.05);\n",
    "    plt.plot(centers[:,k,0], centers[:,k,1], '-', c=color_cycle[k], alpha=0.1);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9079555d-3b47-4a27-bdbb-34f73f2dfc53",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Latent variable models\n",
    "\n",
    "There are two approaches to model correlations in data:\n",
    "* Use **observable variables only**, with complex interactions between them\n",
    "* Use **additional latent variables**  as in **Latent Variable Models** (LVMs)\n",
    "\n",
    "Latent variable models have fewer parameters and require less data to learn, _but_ they are harder to learn.\n",
    "\n",
    "<center> <img src=\"figs/latent_variable_models.png\" width=300></center>\n",
    "\n",
    "Left: $3+8+2+2+2=17$. Right: $3+8+16+32=59$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49eb675-d694-4af1-975b-003f1bbccd78",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Clustering as a problem with latent variables\n",
    "\n",
    "Consider a model:\n",
    "$$p(x,k)=p(k)p(x|k)$$\n",
    "with $x$ observed (continuous or real) and $k=1,\\ldots,K$ a discrete latent variable.\n",
    "The log likelihood is\n",
    "$$\\log L = \\sum_\\mu \\log p(x^\\mu)\\qquad p(x^\\mu)=\\sum_k p(x^\\mu,k)$$\n",
    "\n",
    "The sum expresses that different clusters contribute to the probability of the data point $x^\\mu$.\n",
    "In (hard) clustering, the sum is replaced by the single most contributing term (NB: $k^\\mu$ depends on the parameters of the model and changes during the algorithm):\n",
    "$$p(x^\\mu)=\\sum_k p(x^\\mu,k)\\approx p(x^\\mu,k^\\mu)\\qquad k^\\mu = \\text{argmax}_k p(x^\\mu,k)$$\n",
    "$$\\log L \\approx \\sum_\\mu \\log p(k^\\mu) + \\sum_\\mu \\log p(x^\\mu|k^\\mu)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe764384-2f85-4698-b7d2-50b2afaa7332",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Mixtures of multinoullis\n",
    "\n",
    "A distribution over $d$ dimensional binary vectors $x=(x_1,\\ldots,x_d)$ with $x_i=0,1$.\n",
    "$$p(x|\\theta)=\\sum_{k=1}^K \\pi_k p(x|k) \\qquad p(x|k)=\\prod_{j=1}^d \\mu_{jk}^{x_j}(1-\\mu_{jk})^{1-x_j}$$\n",
    "with $\\theta=\\{\\pi_k,\\mu_{jk}\\}$ and $\\sum_k \\pi_k=1$ and $0 \\le \\mu_{jk}\\le 1$.\n",
    "\n",
    "We derive a clustering algorithm by adding Lagrange multipliers:\n",
    "$$\\log L \\approx \\sum_\\mu \\log \\pi_{k^\\mu} +\\sum_{\\mu,j} \\left(x_j^\\mu \\log \\mu_{jk^\\mu} +(1-x_j^\\mu)\\log (1-\\mu_{jk^\\mu}\\right)+ \\lambda \\left(\\sum_k \\pi_k -1\\right)$$\n",
    "with $k^\\mu = \\text{argmax}_k p(x^\\mu,k)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3e5f95-6b0e-44ba-b23f-a3c53e94bf45",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Mixtures of multinullis\n",
    "\n",
    "Setting the derivatives to zero, we obtain:\n",
    "$$\\frac{\\partial \\log L}{\\partial \\pi_k} =\\frac{1}{\\pi_k}\\sum_\\mu \\delta_{k,k^\\mu} +\\lambda=0\\quad\\rightarrow\\quad \\pi_k=\\frac{N_k}{N}$$with $N_k=\\sum_\\mu \\delta_{k,k^\\mu}$ the number of data samples that are assigned to cluster $k$ and $N=\\sum_k N_k$ the total number of data samples.$$\\frac{\\partial \\log L}{\\partial \\mu_{jk}}=\\frac{1}{\\mu_{jk}}\\sum_\\mu x_j^\\mu \\delta_{k,k^\\mu} -\\frac{1}{1-\\mu_{jk}}\\sum_\\mu (1-x_j^\\mu) \\delta_{k,k^\\mu}$$\n",
    "$$=N_k\\left(\\frac{m_{jk}}{\\mu_{jk}}-\\frac{1-m_{jk}}{1-\\mu_{jk}}\\right)=0 \\quad\\rightarrow\\quad \\mu_{jk}=m_{jk}$$with$$m_{jk}=\\frac{1}{N_k}\\sum_\\mu x_j^\\mu \\delta_{k,k^\\mu}=\\frac{1}{N_k}\\sum_{\\mu \\in k}x_j^\\mu$$\n",
    "the mean of the data that is assigned to cluster $k$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb3f5854-0dd9-4313-b389-8320bf474697",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Mixtures of multinullis: pseudo code\n",
    "\n",
    "```\n",
    "\n",
    "initialize pi_k\n",
    "initialize mu_jk\n",
    "\n",
    "for iter from 1 to max_iter:\n",
    "    \n",
    "    for mu from 1 to N:\n",
    "        k_mu ← argmax(p_mu_k)\n",
    "        \n",
    "    for k from 1 to K:\n",
    "        compute Nk\n",
    "        pi_k ← N_k / N\n",
    "        \n",
    "    for k from 1 to K:\n",
    "        \n",
    "        for j from 1 to d:\n",
    "            compute m_jk\n",
    "            mu_jk ← m_jk\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b38b952-955b-4353-bb98-62d007344d1d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "## Mixture of Bernoullis on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410babcf-ac37-4d53-9b88-c77d613ac0f7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "#### Load MNIST dataset (the lazy way)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1dc16030-6f8a-45d6-a0d8-105054eb54a8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# torch\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# CONSTRUCT MY OWN MNIST LOADER\n",
    "kwargs = {'num_workers': 0, 'pin_memory': False}\n",
    "\n",
    "size = 28\n",
    "\n",
    "# extract all dataset\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST('/home/ai/repos/conv_emerge/data', # change this to your data directory\n",
    "                               train=True,\n",
    "                               download=False,\n",
    "                               transform=transforms.Compose([\n",
    "                                   # transforms.RandomAffine(0, translate=(1,1)),\n",
    "                                   # transforms.Resize(size),\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize((0.1307,), (0.3081,)) # MNIST params\n",
    "                               ])),\n",
    "    batch_size=60000, shuffle=False, **kwargs)\n",
    "\n",
    "_, (X, y) = next(enumerate(train_loader))\n",
    "X.squeeze_();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bc18491-36df-4497-a2b9-31635b4fccf6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# select the first three digits...\n",
    "X = torch.vstack([X[y==0], X[y==1], X[y==2]])\n",
    "y = torch.cat([y[y==0], y[y==1], y[y==2]])\n",
    "perm = np.random.permutation(len(X))\n",
    "X = X[perm]\n",
    "y = y[perm]\n",
    "\n",
    "# ...and extract a smaller chunck and binarize\n",
    "num_data = 1000\n",
    "data = X[:num_data].flatten(1).numpy()\n",
    "data = 1. * (data > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7df847-b4f9-4a78-8ee0-423cb1dff5b1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# have a look at some digits\n",
    "i = 16\n",
    "plt.imshow(data[i].reshape(size,size))\n",
    "print(\"label:\", y[i].item());"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14946ed-9320-4763-ad55-9addcc0e0a0e",
   "metadata": {},
   "source": [
    "#### Fit mixture to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be41ccb6-0dde-4193-9dcf-b0f97f44131b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_mixture = 3\n",
    "num_steps = 20\n",
    "tol = 1e-2\n",
    "seed = 1\n",
    "verbose = True\n",
    "keep_story = True\n",
    "\n",
    "pi = 1/num_mixture * np.ones(num_mixture)\n",
    "mu = np.random.rand(num_mixture, data.shape[-1]) * 0.5 + 0.25\n",
    "\n",
    "# you will write your own implementation of EM\n",
    "# res_bernoulli = mixture_models_code.em_bernoulli(data, num_mixture, num_steps, tol, pi=pi, mu=mu,\n",
    "#                                                   seed=seed, verbose=True, keep_story=keep_story)\n",
    "# pickle.dump(res_bernoulli, open(\"datasets/res_bernoulli.p\", \"wb\" ))\n",
    "\n",
    "# load results obtained with my own EM implementation\n",
    "res_bernoulli = pickle.load(open(\"datasets/res_bernoulli.p\", \"rb\"))\n",
    "resp, mu, logliks, resps, mus, pis = res_bernoulli\n",
    "\n",
    "plt.plot(logliks, '.-')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('log likelihood');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2c2e6c-c342-4a18-afc6-b102489b8d18",
   "metadata": {},
   "source": [
    "#### Visualize mean vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6a09d2-25f9-45d4-9238-21280f036ff6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,4))\n",
    "for k in range(num_mixture):\n",
    "    plt.subplot(1, num_mixture, k + 1)\n",
    "    plt.imshow(mu[k].reshape(size,size));\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8989f229-64d8-43fe-b42f-aae909bebbd5",
   "metadata": {},
   "source": [
    "## Mixtures of multinomials with all 10 digits\n",
    "\n",
    "<center><img src=\"figs/multinullis_murphy.png\" width=400></center>\n",
    "\n",
    "Result is not very good:\n",
    "* Some digits are not represented, others are doubled\n",
    "* Result is not reproducible: different initializations yield different results\n",
    "* Some digits might require multiple clusters (different ways to write a 4)\n",
    "\n",
    "The assumption that all pixels are independent given the cluster is **too simple**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8316e56-ed0a-4489-a51a-430cb1fe29ca",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Mixtures of Gaussians\n",
    "\n",
    "A distribution over $d$ dimensional continuous vectors.\n",
    "$$p(x|\\theta)=\\sum_{k=1}^K \\pi_k \\mathcal{N}(x|\\mu_k,\\Sigma_k)$$\n",
    "with $\\theta=\\{\\pi_k, \\mu_k,\\Sigma_k\\}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c41e53a-1019-442d-bdba-72e1deb81d95",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "#### Let's write a function to draw from a Gaussian Mixture Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d1f6b2b-88fc-4cb1-ac9a-7b93cc5c91c4",
   "metadata": {
    "editable": true,
    "raw_mimetype": "",
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def draw_mgm(mixing_coeff, mus, sigmas, num_samples):\n",
    "    if np.abs(mixing_coeff.sum() - 1.) > 1e-10:\n",
    "        raise ValueError(\"mixing coefficients do not sum to 1!\")\n",
    "    num_mixture = len(mixing_coeff)\n",
    "    if len(mus.shape) == 1:\n",
    "        dim = 1\n",
    "        points = np.zeros(num_samples)\n",
    "    else:\n",
    "        dim = mus.shape[1]\n",
    "        points = np.zeros((num_samples, dim))\n",
    "    coeff_draw = np.random.choice(num_mixture, p=mixing_coeff, size=num_samples)\n",
    "    num_class = np.zeros(num_mixture)\n",
    "    for k in range(num_mixture):\n",
    "        num_class[k] = (coeff_draw == k).sum()\n",
    "        if dim > 1:\n",
    "            points[coeff_draw==k] = np.random.multivariate_normal(mus[k], sigmas[k], size=int(num_class[k]))\n",
    "        else:\n",
    "            points[coeff_draw==k] = mus[k] + sigmas[k] * np.random.randn(int(num_class[k]))\n",
    "    return points, coeff_draw, num_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc44ddeb-c0cc-4fd9-addd-a61ac21e7939",
   "metadata": {},
   "source": [
    "#### Draw data from three Gaussians (Cfr. Figure 9.5 in Bishop's book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1cdc6ef0-a1b0-4678-86d3-14bd10ba7479",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# select number and points, missing coefficients and cluster means\n",
    "num_samples = 500\n",
    "\n",
    "mixing_coeff = np.array([0.5, 0.3, 0.2])\n",
    "\n",
    "mus = np.array([[0.25, 0.45],\n",
    "                [0.5, 0.5],\n",
    "                [0.8, 0.6]])\n",
    "\n",
    "# compute covariance matrices from eigenvectors and eigenvalues\n",
    "UT = np.array([[1, 1],\n",
    "               [-1, 1]]) / np.sqrt(2)\n",
    "\n",
    "lambdas = np.array([[0.2**2,0.05**2],\n",
    "                    [0.05**2, 0.2**2],\n",
    "                    [0.2**2,0.05**2]])\n",
    "\n",
    "sigmas = np.einsum('ki,mk,kj->mij', UT, lambdas, UT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977f3c3b-b677-43cc-afa8-c2355a36b117",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# draw points\n",
    "points, coeff_draw, num_class = draw_mgm(mixing_coeff, mus, sigmas, num_samples)\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "plt.subplot(121)\n",
    "for k in range(3):\n",
    "    ind = coeff_draw==k\n",
    "    plt.scatter(points[ind,0], points[ind,1], alpha=0.5);\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(points[:,0], points[:,1], alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57a5406-2d55-4b5a-b954-ae1950f672d8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Expectation Maximization\n",
    "\n",
    "Consider a model $p(x,k|\\theta)$.\n",
    "The problem is to find $\\theta$ that maximizes the data likelihood on the observed data $x$:\n",
    "$$L(\\theta)=\\sum_x D(x)\\log p(x|\\theta)=\\sum_x D(x) \\log \\sum_k p(x,k|\\theta)$$\n",
    "with $D(x)$ the empirical distribution of $x$ (The empirical distribution is the distribution implied by the data set: $D(x)=\\frac{1}{N}\\sum_{\\mu=1}^N \\delta_{x,x^\\mu}$ and $L(\\theta)=\\frac{1}{N}\\sum_\\mu \\log p(x^\\mu|\\theta)$. The $\\log \\sum$ makes optimization hard.\n",
    "\n",
    "Instead of maximizing $L$, we compute a lower bound and maximize that. For given $x$:\n",
    "$$L_x(\\theta)=\\log \\sum_k p(x,k|\\theta)=\\log \\sum_k q_x(k)\\frac{ p(x,k|\\theta)}{q_x(k)}\\ge \\sum_k q_x(k) \\log \\frac{p(x,k|\\theta)}{q_x(k)}=Q_x(\\theta,q)$$This is called a **variational** or **Jensen bound**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b152c5a-0464-4b15-98fe-aaf24c258dd3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "The bound can be understood in terms of KL divergence:$$L_x(\\theta)-Q_x(\\theta,q)=\\log p(x|\\theta)-\\sum_k q_x(k) \\log \\frac{p(x,k|\\theta)}{q_x(k)}= \\sum_k q_x(k) \\log \\frac{q_x(k)}{p(k|x,\\theta)} =KL(q_x|p(\\cdot|x,\\theta))$$\n",
    "The bound $L_x(\\theta)\\ge Q_x(\\theta,q)$ thus follows also from $KL\\ge 0$.\n",
    "\n",
    "**E step:** Maximizing $Q_x(\\theta_t,q)$ (or minimizing $KL$) with respect to $q_x$ gives\n",
    "$$q_x^*(k)=p(k|x,\\theta_t)\\qquad KL(q_x^*(k)|p(k|x,\\theta_t))=0$$\n",
    "with $\\theta_t$ the parameters at iteration $t$ of the algorithm. $r^\\mu_k= q^*_{x^\\mu}(k)$ is called the **responsibility** of cluster $k$ for data point $x^\\mu$.\n",
    "NB: we have a different distribution $q_x$ for each data point $x$.\n",
    "NB: $KL(q_x^*(k)|p(k|x,\\theta_t))=0$ can only be obtained when variational model $q$ is sufficiently expressive.\n",
    "\n",
    "Substitution gives\n",
    "$$Q_x(\\theta,q_x^*)=\\sum_k p(k|x,\\theta_t)\\log \\frac{p(x,k|\\theta)}{p(k|x,\\theta_t)}$$Note that:$$Q(\\theta_t,q^*)=L(\\theta_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91df3dd5-154c-447d-9ed3-480a091a113c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**M step:** maximize $Q(\\theta)=\\sum_x D(x) Q_x(\\theta,q_x^*)$ wrt $\\theta$.\n",
    "$$\\theta_{t+1} =\\text{argmax}_{\\theta} \\sum_x D(x) Q_x(\\theta,q_x^*)=\\text{argmax}_{\\theta}\\sum_x D(x) \\sum_k p(k|x,\\theta_t)\\log p(x,k|\\theta)$$\n",
    "Note that the $\\sum_k$ now appears outside the log, whereas in the original log likelihood $L(\\theta)=\\sum_x D(x) \\log \\sum_k p(x,k|\\theta)$ it appears inside the log."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ae46f5-09a3-4adc-acee-10ddfc7a9352",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "$\\log p(x|\\theta)$ is concave in $\\theta$ because $p(x,k|\\theta)$ is an exponential model.\n",
    "As a result, the maximization in the M step can be easily computed (for exponential models)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db619658-f37c-4259-8ed8-d8cb45e3cfcb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Convexity of $\\log p(x|\\theta)$ for an exponential model\n",
    "\n",
    "Consider an exponential model\n",
    "$$p(x|\\theta)=\\exp\\left(\\sum_a\\theta_a \\phi_a(x) - \\log Z(\\theta) \\right)\\qquad Z=\\sum_x \\exp\\left(\\sum_a\\theta_a \\phi_a(x) \\right)$$It is easy to show that$$\\frac{\\partial \\log Z}{\\partial \\theta_a}=\\langle\\phi_a\\rangle\\qquad \\frac{\\partial^2 \\log Z}{\\partial \\theta_a\\partial \\theta_b}=\\langle\\phi_a\\phi_b\\rangle-\\langle\\phi_a\\rangle\\langle\\phi_b\\rangle=C_{ab}$$\n",
    "The **covariance matrix** $C$ is positive semi-definite ($\\sum_{ab}v_a C_{ab}v_b\\ge 0$ for all vectors $v_a$). Therefore $\\log Z(\\theta)$ is convex in $\\theta$. Since $\\log p(x|\\theta) =\\sum_a\\theta_a \\phi_a(x) - \\log Z(\\theta)$ we conclude that $p(x|\\theta)$ is concave in $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1329748-84ec-4cae-bc9f-9c69479f7f93",
   "metadata": {},
   "source": [
    "#### EM increases the data log likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbd3a4d-3dd1-48d3-aa04-c8144350afc4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "For subsequent EM steps we have:\n",
    "$$L(\\theta_{t})=Q(\\theta_{t},q^*_t) \\le Q(\\theta_{t+1},q^*_t) \\le Q(\\theta_{t+1},q^*_{t+1}) =L(\\theta_{t+1})$$\n",
    "First and second equality because the model $q^*$ is sufficiently expressive.\n",
    "First inequality because of M-step/maximization.\n",
    "Second inequality because of E-step/Jensen's bound.\n",
    "\n",
    "<center> <img src=\"figs/em_likelihood.png\" width=300></center>\n",
    "\n",
    "Blue: $Q(\\theta,q_t^*)$ with $Q(\\theta_t,q_t^*)=L(\\theta_t)$ (E step). Maximization yields $\\theta_{t+1}$ (M step).\n",
    "Green: $Q(\\theta,q_t^*)$ with $Q(\\theta_{t+1},q_t^*)=L(\\theta_{t+1})$. Maximization yields $\\theta_{t+2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4412532-f1a7-4d5d-8f77-aca26c94803c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Maximum likelihood via EM for mixture of Gaussians\n",
    "The model is\n",
    "$$p(x,k)=p(k)p(x|k)\\qquad p(k) =\\pi_k \\qquad p(x|k)=\\mathcal{N}(x|\\mu_k,\\Sigma_k)$$\n",
    "with parameters $\\pi_k,\\mu_k,\\Sigma_k$.\n",
    "\n",
    "**E step:** For each data point, compute the responsibility\n",
    "$$q_\\mu^*(k)=p(k|x^\\mu,\\theta_t)=\\frac{\\pi_k \\mathcal{N}(x^\\mu|\\mu_k,\\Sigma_k)}{\\sum_{k'} \\pi_{k'} \\mathcal{N}(x^\\mu|\\mu_{k'},\\Sigma_{k'})}=r_{\\mu k}\\qquad k=1,\\ldots, K$$\n",
    "$p(k|x^\\mu,\\theta_t)$ does a 'soft assignment' of data point $x^\\mu$ to Gaussian component $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752028df-22b9-4369-af88-067b0c9446d7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Maximum likelihood via EM for mixture of Gaussians\n",
    "\n",
    "**M step:**\n",
    "$$\\theta_{t+1} =\\text{argmax}_\\theta \\sum_{x}D(x) \\sum_k q^*_x(k) \\log p(x,k|\\theta)$$\n",
    "$$= \\text{argmax}_\\theta \\sum_{\\mu,k} r_{\\mu k} \\left(\\log \\pi_k +\\log \\mathcal{N}(x^\\mu | \\mu_k,\\Sigma_k)\\right)$$with $\\theta_{t+1}=\\{\\pi_k,\\mu_k,\\Sigma_k\\}_{t+1}$. Define $r_k=\\sum_\\mu r_{\\mu k}$. The solution is$$\\pi_k = \\frac{r_k}{N}\\qquad (\\mu_k)_i = \\frac{\\sum_\\mu r_{\\mu k} x_i^\\mu}{r_k}\\qquad \\left(\\Sigma_k\\right)_{ij} =\\frac{\\sum_\\mu r_{\\mu k}x^\\mu_i x^\\mu_j}{r_k}-(\\mu_k)_i(\\mu_k)_j$$\n",
    "This solution makes sense: the cluster mean and covariance are weighted sums of the points assigned to the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc4e4ac-6187-4abd-b0da-b6e992853898",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "## Inference in a MGM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb49b8c-13c1-4d80-8046-f0e73971b8eb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_mixture = 2\n",
    "num_steps = 100\n",
    "tol = 1e-2\n",
    "seed = 1\n",
    "verbose = True\n",
    "keep_story = True\n",
    "init_from_kmeans = False\n",
    "\n",
    "if init_from_kmeans: # init mu from k means\n",
    "    print(\"running k-means\")\n",
    "    # no free lunch: you will use your own kmeans code to initialize EM\n",
    "    mu, sigma, pi = mixture_models_code.init_from_k_means(points_oldfaith, num_mixture, num_steps, tol,\n",
    "                                                          center=None, seed=seed, verbose=True, keep_story=False)\n",
    "    \n",
    "else: # init randomly\n",
    "    pi = 1/num_mixture * np.ones(num_mixture)\n",
    "    mu = np.array([[-1.1,1],\n",
    "                   [1,-1]])\n",
    "    sigma = np.zeros((num_mixture, 2, 2))\n",
    "    for k in range(num_mixture):\n",
    "        sigma[k] = np.eye(2)\n",
    "\n",
    "# you will write your own implementation of EM for MGM\n",
    "# res_mgm = mixture_models_code.em_mgm(points_oldfaith, num_mixture, num_steps, tol, pi=pi, mu=mu, sigma=sigma, seed=seed,\n",
    "#                                      verbose=True, keep_story=keep_story)\n",
    "# pickle.dump(res_mgm, open(\"datasets/res_mgm.p\", \"wb\" ))\n",
    "\n",
    "# load results obtained with my own EM implementation\n",
    "res_mgm = pickle.load(open(\"datasets/res_mgm.p\", \"rb\"))\n",
    "resp, mu, sigma, pi, logliks, resps, mus, sigmas, pis = res_mgm\n",
    "\n",
    "\n",
    "plt.plot(logliks, '.-');\n",
    "plt.xlabel('step')\n",
    "plt.ylabel('loglikelihood');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26114739-c1cb-4fd1-b4a1-414eccf18056",
   "metadata": {},
   "source": [
    "#### Visualize dynamics of EM algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed76a33f-754b-46d6-bcc1-a4829e8a2278",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tot_steps = len(resps)\n",
    "\n",
    "fig, axs = plt.subplots(tot_steps, 1)\n",
    "fig.set_size_inches(10, 3 * tot_steps)\n",
    "for step, (resp, mu) in enumerate(zip(resps, mus)):\n",
    "    for k in range(num_mixture):\n",
    "        axs[step].set_aspect('equal', 'box')\n",
    "        axs[step].plot(mu[k,0], mu[k,1], 'x', ms=8, color='black')\n",
    "        axs[step].scatter(points_oldfaith[:,0], points_oldfaith[:,1],\n",
    "                          alpha=0.1, color=np.vstack([resp[0], np.zeros(resp.shape[1]), resp[1]]).T)\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab35f0e-2718-4ffe-8178-df8e2cd18449",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "#### Draw from infered MGM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea91aa4-d127-4d35-9f3a-760b29af7984",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "points, coeff_draw, num_class = draw_mgm(pi, mu, sigma, 20000)\n",
    "\n",
    "for k in range(3):\n",
    "    ind = coeff_draw==k\n",
    "    plt.scatter(points[ind,0], points[ind,1], alpha=0.1);\n",
    "plt.scatter(points_oldfaith[:,0], points_oldfaith[:,1], color='black', alpha=0.3);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1d7430ad-8a56-43f5-80d2-0b0c88f1fe97",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Variational EM\n",
    "\n",
    "For complex models, finding a 'tabular' solution **$q(z)=p(z|x,\\theta_t)$** in the E step may not always be possible.\n",
    "\n",
    "In Variational EM we  approximate the responsibilities **$p(z|x,\\theta)$** with a variational distribution **$q(z|x,\\phi)$**, where:\n",
    "\n",
    "  - **$q(z|x)=q(z|x,\\phi)$** is a parametrized distribution with parameters **$\\phi$**.\n",
    "  - In the E step, **$q(z|x,\\phi)$** is optimized not for each **$x$** separately, but for all **$x$** simultaneously.\n",
    "\n",
    "$$Q_x(\\theta,q)=\\sum_k q_x(k) \\log \\frac{p(x,k|\\theta)}{q_x(k)} \\quad\\to \\quad Q(\\theta,\\phi)=\\frac{1}{N}\\sum_\\mu \\sum_z q(z|x^\\mu,\\phi) \\log \\frac{p(x^\\mu,z|\\theta)}{q(z|x^\\mu,\\phi)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4f7f95-1fd6-404d-a436-f01c72526b38",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Variational EM\n",
    "\n",
    "In simple tabular EM, the E step yielded the solution that saturated the bound:\n",
    "\n",
    "$$q_x^*(k) =p(k|x,\\theta_t) \\qquad Q(\\theta_t,q^*_x) =L_x(\\theta_t)$$\n",
    "\n",
    "In variational EM this is no longer true:\n",
    "\n",
    "$$q(z|x,\\phi^*) \\ne p(z|x,\\theta_t)\\qquad  Q_x(\\theta_t,\\phi^*) < L(\\theta_t)$$\n",
    "\n",
    "The consequence is that the property that each EM step increases the likelihood is no longer guaranteed (But if not, the bound gets better\\!).\n",
    "\n",
    "Instead, one performs so-called **coordinate ascent** on $Q(\\theta,\\phi)$, by alternating a gradient step in $\\phi$ for fixed $\\theta$ (E) and a gradient step for $\\theta$ for fixed $\\phi$ (M)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5829631-47dd-474f-b929-b137fd1fbf90",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Variational Auto encoder (VAE)\n",
    "\n",
    "Suppose we have some high-dimensional data **$x$** (images for instance) that we want to compress into latent variables **$z$**. We assume a model **$p(x,z|\\theta)=p(z|\\theta)p(x|z,\\theta)$**. This is a generative model that generates images **$x$**:\n",
    "\n",
    "$$z \\sim p(z|\\theta) \\qquad x \\sim p(x|z,\\theta) \\qquad \\text{(decoder)}$$\n",
    "\n",
    "The problem is to estimate **$\\theta$** such that **$p$** generates images that are similar to a given data set by maximizing the log likelihood **$\\sum_\\mu \\log p(x^\\mu|\\theta)$**.\n",
    "\n",
    "<center>\n",
    "  <img src=\"figs/vae_schematic.png\" width=200>\n",
    "    &nbsp; &nbsp; &nbsp; &nbsp;\n",
    "  <img src=\"figs/autoenc.jpg\" width=500>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d32da8-cc62-4987-9d87-9430d253cbbf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Variational Auto encoder (VAE)\n",
    "\n",
    "Model the distribution **$p(x)$** of binary MNIST data.\n",
    "\n",
    "<center><img src=\"figs/vae_schematic.png\" width=200></center>\n",
    "\n",
    "**Decoder:** Generate $z\\sim \\mathcal{N}(z|0,1)$ and $p(x|z,\\theta)$ a MLP with one hidden layer:\n",
    "\n",
    "$$p(x|z,\\theta)=\\prod_{i=1}^{n_x} \\mu_i^{x_i}(1-\\mu_i)^{1-x_i}\\quad \\mu_i=\\sigma\\left(\\sum_{j=1}^{n_h} w^{(1)}_{ij}h_j +w^{(1)}_{i0}\\right)\\quad h_j= \\tanh\\left(\\sum_{k=1}^{n_z} w^{(0)}_{jk}z_k +w^{(0)}_{j0}\\right)$$\n",
    "\n",
    "with **$n_x,n_z$** the dimensions of the **$x,z$** spaces and **$n_h$** the number of hidden units and $\\theta={w^{(0)}_{jk},w^{(1)}_{ij}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c28ee0-507e-4b48-8b16-a312408f26da",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Variational Auto encoder (VAE)\n",
    "\n",
    "Model the distribution **$p(x)$** of continuous Frey face data.\n",
    "\n",
    "<center><img src=\"figs/vae_schematic.png\" width=200></center>\n",
    "\n",
    "**Decoder:** Generate $z\\sim \\mathcal{N}(z|0,1)$ and $p(x|z,\\theta)$ two MLPs with one hidden layer:\n",
    "\n",
    "$$p(x|z,\\theta)=\\prod_{i=1}^{n_x} \\mathcal{N}(x_i|\\mu_i,\\sigma^2_i)\\qquad \n",
    "\\mu_i=\\sum_{j=1}^{n_h} w^{(1)}_{ij} h_j+w^{(1)}_{i0}$$\n",
    "\n",
    "$$\\log \\sigma_i^2 = \\sum_{j=1}^{n_h} w^{(2)}_{ij}h_j +w^{(2)}_{i0}\\qquad h_j= \\tanh \\left(\\sum_{k=1}^{n_z} w^{(0)}_{jk}z_k +w^{(0)}_{j0}\\right)$$\n",
    "\n",
    "with $\\theta=\\{w_{jk}^{(0)},w_{ij}^{(1)}, w_{ij}^{(2)}\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c8e600-f692-4381-bc98-87d4aa170ea4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Variational Auto encoder (VAE)\n",
    "\n",
    "**Encoder:**\n",
    "\n",
    "$$q(z|x,\\phi)=\\prod_{i=1}^{n_z} \\mathcal{N}(z_i|\\mu_i,\\sigma^2_i)\\qquad \n",
    "\\mu_i=\\sum_{j=1}^{n_h} w^{(3)}_{ij} h_j+w^{(3)}_{i0}$$\n",
    "\n",
    "$$\\log \\sigma_i^2 = \\sum_{j=1}^{n_h} w^{(4)}_{ij} h_j +w^{(4)}_{i0}\\qquad \n",
    "h_j = \\tanh\\left(\\sum_{k=1}^{n_x} w^{(5)}_{jk} x_k +w^{(5)}_{j0}\\right)$$\n",
    "\n",
    "with $\\phi=\\{w^{(3)}_{ij},w^{(4)}_{ij},w^{(5)}_{jk}\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0431639-931a-4f39-b39a-d59f0120b59b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Variational Auto encoder (VAE)\n",
    "\n",
    "Training with stochastic gradient descent. **Weight decay** (prior on **$\\theta$**). Number of hidden units is 500 (MNIST) and 200 (Frey face). Plot of Variational lower bound $Q(\\theta,\\phi)$ versus training iteration:\n",
    "\n",
    "<center><img src=\"figs/vae_learning.png\" width=700></center>\n",
    "\n",
    "Bound improves with larger latent space dimension. No overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0720b0-6871-401e-acbb-5a90a5c3f71e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Variational Auto encoder (VAE)\n",
    "\n",
    "<center><img src=\"figs/vae_manifold.png\" width=600></center>\n",
    "\n",
    "Visualization of latent $z$ space in case of $n_z=2$.\n",
    "$z\\sim \\mathcal{N}(z|0,1)$ and $x\\sim p(x|z,\\theta)$ with $\\theta$ the parameters after training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f976fcf-5b60-44ce-be9f-0c4e8c37238e",
   "metadata": {},
   "source": [
    "# <center>Assignments</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92571923-ed2d-4b18-98d0-a0a053933959",
   "metadata": {},
   "source": [
    "#### Ex 7.1\n",
    "\n",
    "Write your own K-means algorithm and compare your results with the ones on the Old Faithful dataset in section **K-means: do it yourself**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c7bb89-4ba8-44e4-97a1-5006e83d975f",
   "metadata": {},
   "source": [
    "#### Ex 7.2\n",
    "\n",
    "Write a clustering algorithm based on the multinomial mixture model and apply it to the MNIST data. Compare your results with the ones showed in section **Mixture of Bernoullis on MNIST**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f15047e-c336-4d17-abb7-685aaeb1df25",
   "metadata": {},
   "source": [
    "#### Ex 7.4\n",
    "\n",
    "Consider the one dimensional Gaussian mixture model\n",
    "$$\n",
    "p(x,k)=\\pi_k \\frac{1}{\\sqrt{2\\pi \\sigma_k^2} }\\exp\\left(-\\frac{(x-a_k)^2}{2\\sigma_k^2}\\right)\n",
    "$$\n",
    "with observable data $x^\\mu, \\mu =1, \\dots, N$ and discrete latent variable $k=1,\\dots,K$.\n",
    "Derive an EM algorithm to estimate the parameters $\\pi_k,a_k, \\sigma_k^2, k=1,\\dots,K$ from the data.\n",
    "Proceed with the following steps.\n",
    "* Give an expression for the responsabilities $r^\\mu_k$ that result from the E step\n",
    "* Give an expression for the variational bound $Q(\\theta,q^*)$ in terms of the responsabilities.\n",
    "* Show that the M-step can be solved in close form and yields new values of $\\pi_k,a_k, \\sigma_k^2, k=1,\\dots,K$ in terms of the responsabilities and the data. Check that your final result agrees with the multi-dimensional presented in the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6497e402-0c9d-4c69-bb4c-a71d01867a3f",
   "metadata": {},
   "source": [
    "#### Ex 7.4\n",
    "\n",
    "Implement EM for a Mixture of Gaussian and compare your results with the one in section **Inference in a MGM**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
